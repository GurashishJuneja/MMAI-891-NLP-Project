{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOpwvAIpo/e5nbSJJQ+WlaJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GurashishJuneja/MMAI-891-NLP-Project/blob/dev/Tony/XLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Initial import of packages and mount**\n"
      ],
      "metadata": {
        "id": "2WUpOP9G7eBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, XLMRobertaXLModel\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "import os,re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "metadata": {
        "id": "pzVd5Cyt6qfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b11a1f-188c-4251-ef7f-0580afcd36f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive to Google Colab notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/NewDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBTAZVSKIz3U",
        "outputId": "8b1ea959-ae3b-4258-b5d8-b84471fb65b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/NewDrive; to attempt to forcibly remount, call drive.mount(\"/content/NewDrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/NewDrive/MyDrive/MMAI 891 NLP Project/Train.csv\")\n",
        "test_df = pd.read_csv(\"/content/NewDrive/MyDrive/MMAI 891 NLP Project/Test.csv\")\n",
        "train_df.info()\n",
        "test_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE_UHD--J4J6",
        "outputId": "0e24c49e-34be-41f4-936a-0f4c2087a64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10001 entries, 0 to 10000\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   tweet_id   10001 non-null  object \n",
            " 1   safe_text  10001 non-null  object \n",
            " 2   label      10000 non-null  float64\n",
            " 3   agreement  9999 non-null   float64\n",
            "dtypes: float64(2), object(2)\n",
            "memory usage: 312.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define valid label values\n",
        "valid_labels = [-1, 0, 1]\n",
        "\n",
        "# filter out rows with invalid label values\n",
        "train_df = train_df[train_df['label'].isin(valid_labels)]\n",
        "\n",
        "# reset the index after filtering\n",
        "train_df = train_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "rfAeQayxEpRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "QbSpBnd2GxkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from train dataframe\n",
        "train_text = train_df['safe_text']\n",
        "val_text = val_df['safe_text']"
      ],
      "metadata": {
        "id": "2ZXbfXt7Ki44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "LGGkSA6aHWW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentences(sentences):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            sent,\n",
        "                            add_special_tokens = True,\n",
        "                            max_length = 40,\n",
        "                            padding = 'max_length',\n",
        "                            truncation=True,\n",
        "                            return_attention_mask = True,\n",
        "                            return_tensors = 'pt'\n",
        "                       )       \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "    \n",
        "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
        "    \n",
        "input_ids, attention_masks = tokenize_sentences(train_df['safe_text'])"
      ],
      "metadata": {
        "id": "0jvkqMvXMkkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'\\(\\@.*?\\)'\n",
        "train_text = [re.sub(pattern, '', text) for text in train_text]\n",
        "val_text = [re.sub(pattern, '', text) for text in val_text]\n"
      ],
      "metadata": {
        "id": "5ZuVJkBIK5r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text normalization\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text) # remove urls\n",
        "    text = re.sub(r'@\\w+', '', text) # remove @users\n",
        "    text = re.sub(r'#\\w+', '', text) # remove hashtags\n",
        "    text = re.sub(r'[^\\w\\s#@/:%.,_-]+', '', text) # remove emoticons\n",
        "    text = re.sub(r'\\d+', '', text) # remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text) # remove extra whitespace\n",
        "    return text\n",
        "\n",
        "normalized_train = [normalize_text(t) for t in train_text]\n",
        "normalized_val = [normalize_text(t) for t in val_text]\n"
      ],
      "metadata": {
        "id": "3djws5aGJVtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unwanted_words = ['url', 'user']\n",
        "def remove_unwanted_words(sentences, unwanted_words):\n",
        "    result = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        filtered_words = [word for word in words if word.lower() not in unwanted_words]\n",
        "        filtered_sentence = \" \".join(filtered_words)\n",
        "        result.append(filtered_sentence)\n",
        "    return result\n",
        "\n",
        "cleaned_train = remove_unwanted_words(normalized_train, unwanted_words)\n",
        "cleaned_val = remove_unwanted_words(normalized_val, unwanted_words)"
      ],
      "metadata": {
        "id": "Y79sf5w0N9Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_labels(label):\n",
        "    if label == -1:\n",
        "        return 0\n",
        "    elif label == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2"
      ],
      "metadata": {
        "id": "DWezsEsUjZlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess train set\n",
        "max_length = 32\n",
        "train_label = train_df['label'].tolist()\n",
        "encoded_inputs_train = tokenizer(cleaned_train, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "train_input_ids = encoded_inputs_train['input_ids']\n",
        "train_attention_mask = encoded_inputs_train['attention_mask']\n",
        "train_labels = [map_labels(label) for label in train_label]\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "\n"
      ],
      "metadata": {
        "id": "mCs41jLQMd5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess validation set\n",
        "val_label = val_df['label'].tolist()\n",
        "encoded_inputs_val = tokenizer(cleaned_val, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "val_input_ids = encoded_inputs_val['input_ids']\n",
        "val_attention_mask = encoded_inputs_val['attention_mask']\n",
        "val_labels = [map_labels(label) for label in val_label]\n",
        "val_labels = torch.tensor(val_labels, dtype=torch.long)"
      ],
      "metadata": {
        "id": "dLfUdIYjHZBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# assuming you have a tensor object called 'tensor_obj'\n",
        "np.unique(val_labels.numpy(), return_counts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkBh5Vy2hh4r",
        "outputId": "12d21f2b-ddc5-4292-be10-02d12aecd1db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 2]), array([240, 943, 817]))"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XLM build up"
      ],
      "metadata": {
        "id": "NqYuIAiD6Nl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "\n",
        "\n",
        "# # preprocess tweets\n",
        "# max_length = 64\n",
        "# batch_size = 128\n",
        "# num_samples = len(train_df)\n",
        "# num_batches = (num_samples + batch_size - 1) // batch_size\n",
        "\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(model.config.hidden_size, 3)\n",
        ")\n"
      ],
      "metadata": {
        "id": "9IoGrWwD7I1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "KdVHYwnQ7rIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up checkpoint saving\n",
        "checkpoint_path = '/content/NewDrive/MyDrive/MMAI 891 NLP Project/XLMcheckpoint.pth'\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    classifier.load_state_dict(checkpoint['classifier_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch_start = checkpoint['epoch'] + 1\n",
        "else:\n",
        "    epoch_start = 1"
      ],
      "metadata": {
        "id": "0Z0pTuWburlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # train on train set\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        logits = model(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
        "        preds = classifier(logits)\n",
        "        loss = criterion(preds, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # evaluate on validation set\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in val_loader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        logits = model(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
        "        preds = classifier(logits)\n",
        "        _, predicted = torch.max(preds.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Validation Accuracy: {acc}')\n",
        "\n",
        "      # save checkpoint\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'classifier_state_dict': classifier.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhuKhDoC75pl",
        "outputId": "6e0d2250-0edc-451c-98db-670f6c1db4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.0038564205169678, Validation Accuracy: 0.4725\n",
            "Epoch 2, Loss: 1.0283615589141846, Validation Accuracy: 0.496\n",
            "Epoch 3, Loss: 0.9423801302909851, Validation Accuracy: 0.5115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load test dataframe"
      ],
      "metadata": {
        "id": "9PR-Zls2XA2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"/content/NewDrive/MyDrive/MMAI 891 NLP Project/Test.csv\")\n",
        "test_df.info()"
      ],
      "metadata": {
        "id": "OhthfdU2WwFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ecc2bc6-5c10-47df-c857-46f392bc16a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5177 entries, 0 to 5176\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   tweet_id   5177 non-null   object\n",
            " 1   safe_text  5176 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 81.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = test_df.dropna(subset=['safe_text'])\n",
        "test_text = train_df['safe_text']\n",
        "test_text = [re.sub(pattern, '', text) for text in test_text]\n",
        "normalized_test = [normalize_text(t) for t in test_text]\n",
        "cleaned_test = remove_unwanted_words(normalized_test, unwanted_words)\n"
      ],
      "metadata": {
        "id": "2FETP3RIhGLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type_counts = {}\n",
        "for element in cleaned_test:\n",
        "    element_type = type(element).__name__\n",
        "    if element_type in type_counts:\n",
        "        type_counts[element_type] += 1\n",
        "    else:\n",
        "        type_counts[element_type] = 1\n",
        "\n",
        "for element_type, count in type_counts.items():\n",
        "    print(f\"{element_type}: {count}\")"
      ],
      "metadata": {
        "id": "zA7UqTqua9St",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d7a3cd8-3e22-42d6-8d18-3235a6a70d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "str: 7999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "float_elements = []\n",
        "for element in cleaned_test:\n",
        "    if isinstance(element, float):\n",
        "        float_elements.append(element)\n",
        "\n",
        "print(float_elements)"
      ],
      "metadata": {
        "id": "svNnyV7tf1cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "755e88a6-b452-426a-c023-daf20aa4ae1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set batch size and initialize empty list for predictions\n",
        "batch_size = 16\n",
        "test_preds = []\n",
        "\n",
        "# loop over batches of test data\n",
        "for i in range(0, len(test_df), batch_size):\n",
        "    # preprocess batch of test tweets and get logits\n",
        "    batch_encoded_inputs = tokenizer(cleaned_test, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "    batch_input_ids = batch_encoded_inputs['input_ids']\n",
        "    batch_attention_mask = batch_encoded_inputs['attention_mask']\n",
        "    batch_outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
        "    batch_logits = batch_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    # get batch predictions and add to list\n",
        "    batch_preds = classifier(batch_logits).argmax(dim=1).tolist()\n",
        "    test_preds += batch_preds\n",
        "\n",
        "# save predictions to file\n",
        "output_df = pd.DataFrame({'ID': test_df['tweet_id'], 'target': test_preds})\n",
        "output_df.to_csv('/content/NewDrive/MyDrive/MMAI 891 NLP Project/predictions.csv', index=False)\n"
      ],
      "metadata": {
        "id": "6b-w2q8NXqoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df.info()"
      ],
      "metadata": {
        "id": "UPkGb4p61g7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts = output_df['target'].value_counts()\n",
        "print(value_counts)"
      ],
      "metadata": {
        "id": "mI1iYMeE1i2z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}